{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "## **Notebook 1: Introduction to CuPy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c50a5376-6bd0-4d23-b61a-7a2954abab6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Introduction to CuPy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbcc303-3c50-45c6-82f1-e7068e6d0665",
   "metadata": {},
   "source": [
    "NumPy is a widely used library for numerical computing in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf823dc-3fef-431f-9abb-ee827c0cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = np.random.randn(size, size)\n",
    "\n",
    "%timeit -n 5 Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c7dcb8c-7141-46f3-bd08-ea07261e9e65",
   "metadata": {},
   "source": [
    "CuPy uses a NumPy-like interface. Porting a Numpy code to CuPy can be as simple as changing your import statement. In this workshop, we'll always use `import cupy as cp` for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d196d0-c618-4805-863a-3b57e873d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = cp.random.randn(size, size)\n",
    "\n",
    "Q, R = cp.linalg.qr(A)\n",
    "%timeit -n 5 Q, R = cp.linalg.qr(A) ; cp.cuda.Device().synchronize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e954a9de-3519-400e-8a44-d59668bd26f2",
   "metadata": {},
   "source": [
    "We already see a substantial speedup with no real code changes! \n",
    "\n",
    "Notice the additional call to `cp.cuda.Device().synchronize()` in the CuPy version. GPU kernel calls are asynchronous with respect to the CPU. Our call to `synchronize()` ensures the GPU finishes to completion, so we can accurately measure  the elapsed time. We don't generally need to add these calls to production CuPy codes.\n",
    "\n",
    "NumPy is typically used to perform computations on _arrays_ of data. The data is stored in the `numpy.ndarray` object. CuPy implements a similar class called the `cupy.ndarray`. But while the `numpy.ndarray` data resides in host memory, the contents of a `cupy.ndarray` persistent in GPU memory. CuPy provides several helper functions to convert between Cupy and NumPy `ndarrays` - facilitating data transfer to/from the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4b14c-fe6e-4ba7-82fc-0c50219e1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the data on the host\n",
    "A_cpu = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "print(\"A_cpu is a\", type(A_cpu))\n",
    "print(\"With initial values:\\n\", A_cpu)\n",
    "\n",
    "#Copy data, host to device\n",
    "A_gpu = cp.asarray(A_cpu)\n",
    "print(\"A_gpu is a\", type(A_gpu))\n",
    "\n",
    "#Square the data on the device\n",
    "A_gpu = cp.square(A_gpu)\n",
    "\n",
    "#Copy data, device to host\n",
    "A_cpu = cp.asnumpy(A_gpu)\n",
    "\n",
    "print(\"Squared values:\\n\", A_cpu)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2",
   "metadata": {},
   "source": [
    "Note that NumPy and CuPy ndarrys are not implicitly convertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eacf74-5f07-42b0-9b6f-8af7a302d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.square(A_cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc84591b-2329-44a8-a266-baa30866086b",
   "metadata": {},
   "source": [
    "The GPU is a powerhouse of parallel computing performance, and can process math operations much more quickly than the CPU. This is easy to see by comparing performance of CuPy vs NumPy, particularly for dense linear algebra operations. Let's look at a multiplication of 4096x4096 matrices. Notice the similarity of the two versions of the code (NumPy and CuPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64595b-a9b7-43c4-a5ed-87b579aca4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from time import perf_counter\n",
    "\n",
    "size = 4096\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_cpu = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B_cpu = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C_cpu = np.matmul(A_cpu,B_cpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_cpu\n",
    "del B_cpu\n",
    "del C_cpu\n",
    "\n",
    "\n",
    "\n",
    "A_gpu = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B_gpu = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu) #Exclude one-time JIT overhead\n",
    "start_time = perf_counter( )\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "cp.cuda.Device(0).synchronize()\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_gpu\n",
    "del B_gpu\n",
    "del C_gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1d9b000-e8fa-4f67-a219-4640aea54abf",
   "metadata": {},
   "source": [
    "The GPU's strenghts in computational throughput and memory bandwidth can lead to terrific application speedups. But we need to be considerate of two types of overhead when evaluating our problem for acceleration on the GPU with CuPy: kernel overhead, and data movement overhead.\n",
    "\n",
    "---\n",
    "\n",
    "**Kernel Overhead**\n",
    "\n",
    "CuPy compiles kernel codes on-the-fly using JIT compilation. Therefore, there is a compilation overhead the first time a given function is called with CuPy. The compiled kernel code is cached, so compilation overhead is avoided for subsequent executions of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a0d2b-da82-4e46-999c-55bbad4c0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "size = 512\n",
    "for _ in range(5):\n",
    "    A = cp.random.randn(size, size).astype(np.float32)\n",
    "    t1 = time.time()\n",
    "    cp.linalg.det(A)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    t2 = time.time()\n",
    "    print('%.4f' % (t2 - t1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "827d8d9b-e4b2-4376-a441-b6bbf9a78af8",
   "metadata": {},
   "source": [
    "You may also notice a one-time overhead upon first calling a CuPy function in a program. This overhead is associated with the creation of a CUDA context by the CUDA driver, which happens the first time any CUDA API is invoked in a program.\n",
    "\n",
    "In addition, there is a CUDA kernel launch overhead that is penalized each time a GPU kernel is launched. The overhead is on the order of a few microseconds. For this reason, launching many small CUDA kernels in an application will generally lead to poor performance. The kernel launch overhead may dominate your runtime for very small problems, but for large datasets the overhead will be small compared to the actual GPU computation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061b6b0-169f-4e64-8556-b3fd86b97dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "    print(\"\\nInput Matrix size: %d\" % size, \"x %d \" % size)\n",
    "    for xp in [np, cp]:\n",
    "        A=xp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(xp.float32)\n",
    "        xp.linalg.qr(A)#Exclude potential one-time JIT overhead\n",
    "        t1 = time.time()\n",
    "        xp.linalg.qr(A)\n",
    "        cp.cuda.Device().synchronize()\n",
    "        t2 = time.time()\n",
    "        print(xp.__name__, '%f' % (t2 - t1))\n",
    "        del A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff335660-dfa6-4657-9489-7d31bb491d93",
   "metadata": {},
   "source": [
    "It's clear that increasing the problem size can help amoritize the overhead of launching GPU kernels. Another common strategy is to merge multiple kernels together into a single combined kernel, reducing the total number of kernel launches in your program. CuPy supports kernel fusion in this manner via the `@cupy.fuse()` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b1c62-62f6-4da9-a3de-3e5afd5beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "@cp.fuse\n",
    "def fused_squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "x = cp.arange(size)\n",
    "y = cp.arange(size)[::-1]\n",
    "\n",
    "%timeit -n 10 squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "%timeit -n 10 fused_squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "\n",
    "del x\n",
    "del y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b36e7785-3b82-4e6e-9742-76b1caed35c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Data Movement Overhead**\n",
    "\n",
    "Try to minimize data movement to or from the GPU. The FLOP rate and memory bandwidth of a GPU can process data much more quickly than it can be fed with data over the PCIe bus. This problem is being tackled with novel interconnect technologies like NVLink. But it's a real inbalance we have to deal with for now.\n",
    "Let's look at an example where we initialize our input data GPU and then computes the dot product. Note that the result of the multiplication, the C matrix, is available on the GPU in case we need it later.\n",
    "\n",
    "Notice again the similarity of the two parts of the code (NumPy and CuPy). They are virtually identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb0179-8137-4827-8eb2-2c960967593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    C_cpu = np.dot(A_cpu,B_cpu)\n",
    "    stop_time = perf_counter( )\n",
    "    cpu_time = stop_time - start_time\n",
    "    print('numpy = %g seconds' % cpu_time )\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "    A_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    B_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "087ea831-f9e3-41c1-888d-8fed77d9fd1f",
   "metadata": {},
   "source": [
    "But what if the input data for the `dot` operation resides in the system memory? We need to move the data over the PCIe bus (from the host to the GPU) using `cp.asarray()`. \n",
    "\n",
    "Modify the following cell to initialize the ndarray data with Numpy. \n",
    "\n",
    "How does the speedup change after the additional cost of data movement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be63e60-fb09-4936-886d-7e91aebacd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "#>>>Insert CuPy code here\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72fc692d-137a-44ab-8743-3a2fa3ff60bc",
   "metadata": {},
   "source": [
    "Click the `...` below to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a42a53-e98c-4434-95ab-2114ee4d6dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    \n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    \n",
    "    A_gpu=cp.asarray(A_cpu)\n",
    "    B_gpu=cp.asarray(B_cpu)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82076ac4-4ec3-4913-a891-87f029760155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
