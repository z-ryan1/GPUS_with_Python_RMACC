{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9ec28c-395d-4e29-a716-d2a35fb1465c",
   "metadata": {},
   "source": [
    "## **Notebook 2: Introduction to RAPIDS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f01413-6418-453a-a705-8e07e5bc4a88",
   "metadata": {},
   "source": [
    "The following content is from:\n",
    "    https://github.com/rapidsai-community/notebooks-contrib/blob/main/getting_started_materials/intro_tutorials_and_guides/01_Introduction_to_RAPIDS.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2a5c6-3ec8-4791-9ea2-5f11018c05bb",
   "metadata": {},
   "source": [
    "Let's load some helper functions from matplotlib and configure the Jupyter Notebook for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00f5fe-d697-4926-a734-232cf8f12f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e96aa-d988-4669-9859-e48a4396121a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**GPU-Accelerated Data Manipulation with CuDF**\n",
    "\n",
    "*Pandas*\n",
    "\n",
    "Data scientists typically work with two types of data: unstructured and structured. Unstructured data often comes in the form of text, images, or videos. Structured data - as the name suggests - comes in a structured form, often represented by a table or CSV. We'll focus the majority of these tutorials on working with these types of data.\n",
    "\n",
    "There exist many tools in the Python ecosystem for working with structured, tabular data but few are as widely used as Pandas. Pandas represents data in a table and allows a data scientist to manipulate the data to perform a number of useful operations such as filtering, transforming, aggregating, merging, visualizing and many more.\n",
    "\n",
    "For more information on Pandas, check out the excellent documentation: http://pandas.pydata.org/pandas-docs/stable/\n",
    "\n",
    "Below we show how to create a Pandas DataFrame, an internal object for representing tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb2985-469d-4cb3-800a-68336aef437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; print('Pandas Version:', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deb4b7-2a39-4869-a1a4-a6e5139bc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a Pandas DataFrame with\n",
    "# two columns named \"key\" and \"value\"\n",
    "df = pd.DataFrame()\n",
    "df['key'] = [0, 0, 2, 2, 3]\n",
    "df['value'] = [float(i + 10) for i in range(5)]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabce725-b94f-4e32-ac3c-aacffab3b398",
   "metadata": {},
   "source": [
    "We can perform many operations on this data. For example, let's say we wanted to sum all values in the in the value column. We could accomplish this using the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d968e74-8f19-432b-9f15-e8b2422304bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = df['value'].sum()\n",
    "print(aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230f10d-8161-4f20-92d1-6609964609ad",
   "metadata": {},
   "source": [
    "*cuDF*\n",
    "\n",
    "Pandas is fantastic for working with small datasets that fit into your system's memory. However, datasets are growing larger and data scientists are working with increasingly complex workloads - the need for accelerated compute arises.\n",
    "\n",
    "cuDF is a package within the RAPIDS ecosystem that allows data scientists to easily migrate their existing Pandas workflows from CPU to GPU, where computations can leverage the immense parallelization that GPUs provide.\n",
    "\n",
    "Below, we show how to create a cuDF DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec09cb-7d29-498e-b185-0da9399bd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf; print('cuDF Version:', cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3ce7a-96a3-4da6-8add-51740b148faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a cuDF DataFrame with\n",
    "# two columns named \"key\" and \"value\"\n",
    "df = cudf.DataFrame()\n",
    "df['key'] = [0, 0, 2, 2, 3]\n",
    "df['value'] = [float(i + 10) for i in range(5)]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddfb8d-c708-4f6c-b443-7d031e948bb6",
   "metadata": {},
   "source": [
    "As before, we can take this cuDF DataFrame and perform a sum operation over the value column. The key difference is that any operations we perform using cuDF use the GPU instead of the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d761900-b840-4de3-9338-1c84099e6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = df['value'].sum()\n",
    "print(aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bff466-6550-4181-aec9-f44339c35123",
   "metadata": {},
   "source": [
    "Note how the syntax for both creating and manipulating a cuDF DataFrame is identical to the syntax necessary to create and manipulate Pandas DataFrames; the cuDF API is based on the Pandas API. This design choice minimizes the cognitive burden of switching from a CPU based workflow to a GPU based workflow and allows data scientists to focus on solving problems while benefitting from the speed of a GPU!\n",
    "\n",
    "*Comparing the Performance of CuDF and Pandas*\n",
    "\n",
    "Let's try increasing our data size a bit, giving our system more numbers to crunch through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78488ce-b6cb-45d0-a4d1-2f78d2844193",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "pandas_df = pd.DataFrame({'a': np.random.randint(0, 100000000, size=10000000),\n",
    "                          'b': np.random.randint(0, 100000000, size=10000000)})\n",
    "\n",
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d065428-0f60-4fc2-869a-65577db82fcf",
   "metadata": {},
   "source": [
    "Now our dataset is just over 150 MB in size. A very modest size for the GPU, but still show us great speedups with reasonable completion time. Try increasing the problem size for even bigger GPU speedups, but note that the operations can quickly become intractable for the CPU to complete.\n",
    "\n",
    "For now, let's replicate the dataframe on the GPU. To construct the cudf dataframe, we could simply used the `pandas_df` dataframe directly.\n",
    "\n",
    "`cudf_df = cudf.DataFrame.from_pandas(pandas_df)`\n",
    "\n",
    "But instead, let's initialize our data with CuPy to show off the interoperability with CuDF dataframes! Modify the following cell to initialize the dataframe with CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0990b48-3fec-4bf3-a02a-8b9ab6a967f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#pandas_df = pd.DataFrame({'a': np.random.randint(0, 100000000, size=10000000),\n",
    "#                          'b': np.random.randint(0, 100000000, size=10000000)})\n",
    "\n",
    "cudf_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee14185-6517-4c1b-aa9f-d6b177875760",
   "metadata": {},
   "source": [
    "Click the `...` below to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397c098-aa48-4466-846f-79f8aae36d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import cupy as cp\n",
    "\n",
    "cudf_df = cudf.DataFrame({'a': cp.random.randint(0, 100000000, size=10000000),\n",
    "                          'b': cp.random.randint(0, 100000000, size=10000000)})\n",
    "\n",
    "cudf_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282172b5-89a2-4c59-b3f0-743ff8cd59cc",
   "metadata": {},
   "source": [
    "Notice how quickly the CuDF dataframe was created compared to the Pandas dataframe. Now that we have our dataframes in both CPU and GPU memory, let's do some data analysis and manipulation to see see what kind of speedups are possible with GPU acceleration!\n",
    "\n",
    "First, let's try some operations that will touch every data element in the dataframe. A great setup for parallel GPU processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9c09d-372c-49d9-9cf7-49d713ed8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pandas_df.a.mean()\n",
    "%timeit cudf_df.a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc9d3b-7fe3-4bdd-9810-102ef479758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pandas_df.a.max()\n",
    "%timeit cudf_df.a.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e12820-e433-44e8-a963-77c99a66e050",
   "metadata": {},
   "source": [
    "Let's look at some sort and merge operations, which are often computationally expensive and can really show off the power of CuDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd51c5-288f-44da-b6ce-a4a54ea9f7d0",
   "metadata": {},
   "source": [
    "_Merge_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b6c8f-1e19-4fdb-af8b-3fd37e22e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pandas_df.merge(pandas_df, on='b')\n",
    "%timeit cudf_df.merge(cudf_df, on='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5175e2-bfd8-4514-b0c1-01db202d8eea",
   "metadata": {},
   "source": [
    "_Sort_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec238b-7e5c-42a5-8d1c-8e3e69c567a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pandas_df.sort_values('b')\n",
    "%timeit cudf_df.sort_values('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fccd2-1dcf-49b1-ae4d-ee947766641a",
   "metadata": {},
   "source": [
    "_GroupBy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fd343-abee-4f3c-ae80-bd9ebb46675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pandas_df.groupby(by=['b']).sum()\n",
    "%timeit cudf_df.groupby(by=['b']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdfbdc-41b8-4c5a-b3fc-b6177fbaf79f",
   "metadata": {},
   "source": [
    "*A Quick Note on Data Interoperability*\n",
    "\n",
    "The data stored in CuDF dataframes is interoperable with major data analytics and deep learning frameworks in the Python ecosystem. Including Tensorflow, Pytorch, CuPy, and the other tools we'll be talking about next - Numba. This interoperability is largely made possible by DLPack - a standardized specification for tensor structures. And shuttling the data between different applications is done without any memory copies! This will become increasingly powerful as you start to learn more about the possibilities to integrate GPUs into your own workflows! Let's make a quick pit stop to get hands-on experience with DLPack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f952e-615a-479d-ad22-a33b9ac2b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "cp_array = cp.array([100, 200, 300], cp.int32)\n",
    "\n",
    "#Convert  cp.ndarray -> DLpack\n",
    "dlp_array = cp_array.toDlpack()\n",
    "\n",
    "#Convert DLpack -> cudf.dataframe\n",
    "cudf_df = cudf.from_dlpack(dlp_array)\n",
    "print(cudf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae0676-21d9-43be-b6a5-c5e5f72da341",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**GPU-Accelerated Machine Learning with CuML**\n",
    "\n",
    "*Scikit-Learn*\n",
    "\n",
    "After our data has been preprocessed, we often want to build a model so as to understand the relationships between different variables in our data. Scikit-Learn is an incredibly powerful toolkit that allows data scientists to quickly build models from their data. Below we show a simple example of how to create a Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713588d-2211-4256-bbb7-b99f01e32c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; print('NumPy Version:', np.__version__)\n",
    "\n",
    "\n",
    "# create the relationship: y = 2.0 * x + 1.0\n",
    "n_rows = 1000000 # let's use 1M data points\n",
    "w = 2.0\n",
    "x = np.random.normal(loc=0, scale=1, size=(n_rows,))\n",
    "b = 1.0\n",
    "y = w * x + b\n",
    "\n",
    "# add a bit of noise\n",
    "noise = np.random.normal(loc=0, scale=2, size=(n_rows,))\n",
    "y_noisy = y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448db2d9-6b3b-4a87-87ec-bd3c0d20204c",
   "metadata": {},
   "source": [
    "We can now visualize our data using the matplotlib library. Let's plot the first 10,000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e7306-6842-4e86-8beb-408c38164d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:10000], y_noisy[:10000], label='empirical data points')\n",
    "plt.plot(x, y, color='black', label='true relationship')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5e5df-24e7-43ac-8155-a2eac23f797f",
   "metadata": {},
   "source": [
    "We'll use the LinearRegression class from Scikit-Learn to instantiate a model and fit it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42560b-f3bc-44d1-910c-4dd2ff01ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# instantiate and fit model\n",
    "linear_regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268c4d4-a25a-4d9e-9fc8-60e0f17a44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_regression.fit(np.expand_dims(x, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f610f94-9e31-4c9c-9481-b866d0599c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data and perform inference\n",
    "inputs = np.linspace(start=-5, stop=5, num=1000)\n",
    "outputs = linear_regression.predict(np.expand_dims(inputs, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d2cd4-0f75-418d-8966-f1819e68be2e",
   "metadata": {},
   "source": [
    "Let's now visualize our empirical data points, the true relationship of the data, and the relationship estimated by the model. Looks pretty close!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb6a36-754c-4c0f-8bd2-fa4ea080feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:10000], y_noisy[:10000], label='empirical data points')\n",
    "plt.plot(x, y, color='black', label='true relationship')\n",
    "plt.plot(inputs, outputs, color='red', label='predicted relationship (cpu)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25faf3-0d0b-4af3-8f6c-1022d2a98ca5",
   "metadata": {},
   "source": [
    "*cuML*\n",
    "\n",
    "The mathematical operations underlying many machine learning algorithms are often matrix multiplications. These types of operations are highly parallelizable and can be greatly accelerated using a GPU. cuML makes it easy to build machine learning models in an accelerated fashion while still using an interface nearly identical to Scikit-Learn. The below shows how to accomplish the same Linear Regression model but on a GPU.\n",
    "\n",
    "First, let's convert our data from a NumPy representation to a cuDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663b8c8-a6d1-457a-bccd-1ee4b81092a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cuDF DataFrame\n",
    "import cudf\n",
    "df = cudf.DataFrame({'x': x, 'y': y_noisy})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1c5f7-d180-47d0-9d61-d0e7f71c6258",
   "metadata": {},
   "source": [
    "Next, we'll load the GPU accelerated LinearRegression class from cuML, instantiate it, and fit it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea287ae-548d-422c-b32b-cda86bd38a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml; print('cuML Version:', cuml.__version__)\n",
    "from cuml.linear_model import LinearRegression as LinearRegression_GPU\n",
    "\n",
    "\n",
    "# instantiate and fit model\n",
    "linear_regression_gpu = LinearRegression_GPU(algorithm = \"svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029760b-59ed-4119-923f-0889d7ee5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_regression_gpu.fit(df[['x']], df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f69ff4-1c51-4e40-aa2d-a73c1a6d4c03",
   "metadata": {},
   "source": [
    "We can use this model to predict values for new data points, a step often called \"inference\" or \"scoring\". All model fitting and predicting steps are GPU accelerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116ef2c-b64c-45d3-9921-df4d2a43e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data and perform inference\n",
    "new_data_df = cudf.DataFrame({'inputs': inputs})\n",
    "outputs_gpu = linear_regression_gpu.predict(new_data_df[['inputs']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f061d8e-b716-4714-8b01-481b27d268da",
   "metadata": {},
   "source": [
    "Lastly, we can overlay our predicted relationship using our GPU accelerated Linear Regression model (green line) over our empirical data points (light blue circles), the true relationship (blue line), and the predicted relationship from a model built on the CPU (red line). We see that our GPU accelerated model's estimate of the true relationship (green line) is identical to the CPU based model's estimate of the true relationship (red line)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c479e25-643d-4324-af81-22ee7e0fdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:10000], y_noisy[:10000], label='empirical data points')\n",
    "plt.plot(x, y, color='black', label='true relationship')\n",
    "plt.plot(inputs, outputs, color='red', label='predicted relationship (cpu)')\n",
    "plt.plot(inputs, outputs_gpu.to_numpy(), color='green', label='predicted relationship (gpu)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828a9d4-9bb9-4123-8b3e-c3cc71139314",
   "metadata": {},
   "source": [
    "*Comparing the Performance of CuML and Scikit-Learn*\n",
    "\n",
    "To finish out this section, let's look at the GPU-accelerated performance speedups for some commonly used machine learning algorithms. CuML has built-in benchmarking capability to help measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18d72b-8b04-4650-9c03-585e3ffe803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "\n",
    "from cuml.benchmark.runners import SpeedupComparisonRunner\n",
    "from cuml.benchmark.algorithms import algorithm_by_name\n",
    "\n",
    "runner = cuml.benchmark.runners.SpeedupComparisonRunner(\n",
    "    bench_rows=[2**x for x in range(11, 14)], \n",
    "    bench_dims=[256],\n",
    "    dataset_name=\"blobs\",\n",
    "    input_type=\"numpy\")\n",
    "\n",
    "algorithms = [\"NearestNeighbors\", \n",
    "              \"DBSCAN\", \n",
    "              \"LinearRegression\", \n",
    "              \"PCA\", \n",
    "              \"RandomForestClassifier\"]\n",
    "\n",
    "for algorithm in algorithms:   \n",
    "    runner.run(algorithm_by_name(algorithm), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860951c-bb1f-4669-ba25-c8d832fc0f0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053dc1d-bc5b-4c50-9948-18250ae42438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
